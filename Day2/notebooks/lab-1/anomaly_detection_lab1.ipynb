{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection\n",
    "\n",
    "Anomaly or outlier detection is essentially finding patterns that do not conform to expected behavior. There are several approaches to anomaly detection that are based on either statistical properties, clustering, classification, Principal Component Analysis (PCA), or subsampling. In this notebook we will look at an **autoencoder network** for anomaly or outlier detection. An autoencoder is a neural-net based, unsupervised learning model that is used to learn low-dimensional features that captures some structure underlying the high-dimensional input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Scenario\n",
    "\n",
    "Groundwater level is an important metric, especially for agriculture states such as Iowa. One of the metrics [U.S. Geological Survey (USGS)](https://www.usgs.gov/) monitors is **depth to water level in feet below the land**. In this lab we will use a synthetic dataset that models certain scenarios for Des Moines, Iowa. The three key weather-related metrics we will be using are:\n",
    "\n",
    "- water-level (depth to water level in feet below the land)\n",
    "- temperature\n",
    "- humidity\n",
    "\n",
    "The data is generated daily using realistic monthly averages for Des Moines, Iowa, for the years 2016 – 2019.\n",
    "\n",
    "We are going to be using 3 copies of the dataset for years 2016 -2019: \n",
    "\n",
    "1. Normal conditions for the region.\n",
    "2. A sudden precipitous rainfall abnormally raising ground water levels that occurs in mid-May 2019.\n",
    "3. A gradual build up dry conditions dropping the ground water levels that occur over the months of June and July 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. **Introduction to the datasets**: Understand the patterns in the three datasets – normal, sudden and gradual.\n",
    "\n",
    "2. **Define and train the Autoencoder Network**: Use Keras to define and train the autoencoder model.\n",
    "\n",
    "3. **Establish criteria for anomalies**: Define approaches and thresholds for detecting anomalies based on the trained autoencoder model.\n",
    "\n",
    "4. **Predict anomalies**: Used in the trained autoencoder model, make predictions on the sudden and gradual dataset to identify anomalies.\n",
    "\n",
    "5. **Principal Component Analysis**: Apply PCA on the encoded dataset and visualize the data representation at lower dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(293)\n",
    "import math\n",
    "from IPython.display import display, HTML, Image, SVG\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"pandas version: {} numpy version: {}\".format(pd.__version__, np.__version__))\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "print(\"keras version: {} tensorflow version: {} sklearn version: {}\".format(keras.__version__, \n",
    "                                                                        tensorflow.__version__, sklearn.__version__))\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print('importing libraries done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper method to display a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe(df_in):\n",
    "    s = df_in.style.set_properties(**{'text-align': 'left'})\n",
    "    s.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "    display(HTML(s.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the three datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "              'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/normal.xlsx')\n",
    "\n",
    "sudden_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "              'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/sudden.xlsx')\n",
    "\n",
    "gradual_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "               'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/gradual.xlsx')\n",
    "\n",
    "normal_df = pd.read_excel(normal_url)\n",
    "sudden_df = pd.read_excel(sudden_url)\n",
    "gradual_df = pd.read_excel(gradual_url)\n",
    "\n",
    "print('Size of dataset: {} rows'.format(len(normal_df)))\n",
    "print('Done loading datasets!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pair Plots\n",
    "\n",
    "Visualize the pair-wise relationships in the three data sets for the three-key metrics: (1) temperature, (2) humidity, and (3) water-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normal Dataset\n",
    "\n",
    "The water-level appears to be negatively skewed distribution with the peak value of 20 feet. It is important to note that that **higher water-level number represents dry conditions** and **lower water-level number represents wet conditions**. \n",
    "\n",
    "No natural clusters between water-level and temperature, whereas two distinct clusters between water-level and humidity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['temperature', 'humidity', 'water_level']\n",
    "sns.pairplot(normal_df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sudden Dataset\n",
    "\n",
    "Note that in this dataset we have introduced sudden precipitous rainfall that suddenly and temporarily raised ground water levels around mid-May 2019 till end of May 2019.\n",
    "\n",
    "No visible change in the pair wise distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(sudden_df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradual Dataset\n",
    "\n",
    "Note that this dataset emulates gradual build up dry conditions over the months of June and July 2019 that unseasonably dropped the water levels.\n",
    "\n",
    "No visible change in the pair wise distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(gradual_df[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Water Level Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Box and Whisker Plot\n",
    "\n",
    "In a box and whisker plot: the ends of the box are the upper and lower quartiles, so the box spans the interquartile range. the median is marked by a horizontal line inside the box. The whiskers are the two lines outside the box indicating variability outside the upper and lower quartiles. Outliers are plotted as individual points.\n",
    "\n",
    "The plot shows what is normal changes from month to month. The summer months of June and July are relatively wet month from ground water level perspective presumably due to precipitations from earlier months. Maintaining reliable monthly water levels is critical to the agricultural needs.\n",
    "\n",
    "In this visualization, there is not obvious evidence of the induced anomalies in the sudden dataset. The gradual dataset does show a noticeably bigger spread of the months of June and July."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(3, 1, sharey=True, sharex=True, figsize=(6, 7))\n",
    "\n",
    "ax1 = sns.boxplot(x=\"month_name\", y=\"water_level\", data=normal_df, ax=axes[0])\n",
    "ax2 = sns.boxplot(x=\"month_name\", y=\"water_level\", data=sudden_df, ax=axes[1])\n",
    "ax3 = sns.boxplot(x=\"month_name\", y=\"water_level\", data=gradual_df, ax=axes[2])\n",
    "\n",
    "ax1.set_title('Normal Dataset')\n",
    "ax2.set_title('Sudden Dataset')\n",
    "ax3.set_title('Gradual Dataset')\n",
    "ax1.set_xlabel('')\n",
    "ax2.set_xlabel('')\n",
    "ax3.set_xlabel('')\n",
    "ax1.set_ylabel('')\n",
    "ax2.set_ylabel('Water Level')\n",
    "ax3.set_ylabel('')\n",
    "\n",
    "f.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daily Trend Plot\n",
    "\n",
    "This plot shows daily `Water Level` reading for years 2016-2019 (1461 days).\n",
    "\n",
    "In this historic view of the data, you can observer the induced anomalies (around day 1230) in the sudden and the gradual datasets. In the sudden dataset, the `Water Level` suddenly rises, and in the gradual dataset, the `Water Level` gradually drops over a period of time.\n",
    "\n",
    "In this notebook we are going to develop an anomaly detection model that allows us to detect such anomalies in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(3, 1, sharey=True, sharex=True, figsize=(6, 7))\n",
    "ax[0].plot(normal_df.water_level)\n",
    "ax[1].plot(sudden_df.water_level)\n",
    "ax[2].plot(gradual_df.water_level)\n",
    "ax[0].set_title('Normal Dataset')\n",
    "ax[1].set_title('Sudden Dataset')\n",
    "ax[2].set_title('Gradual Dataset')\n",
    "ax[1].set_ylabel('Water Level')\n",
    "f.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train the Autoencoder Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select **month**, **temperature**, **humidity**, and **water level** as our features for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['month', 'temperature', 'humidity', 'water_level']\n",
    "categorical = ['month']\n",
    "numerical = ['temperature', 'humidity', 'water_level']\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())])) for f in numerical]\n",
    "    \n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = categorical_transformations + numeric_transformations\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations))])\n",
    "\n",
    "X_train = clf.fit_transform(normal_df[feature_cols])\n",
    "np.random.shuffle(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Autoencoder Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(10)\n",
    "set_random_seed(10)\n",
    "act_func = 'elu'\n",
    "\n",
    "input_ = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(100, activation=act_func)(input_)\n",
    "x = Dense(50, activation=act_func)(x)\n",
    "encoder = Dense(20, activation=act_func, name='feature_vector')(x)\n",
    "x = Dense(50, activation=act_func)(encoder)\n",
    "x = Dense(100, activation=act_func)(x)\n",
    "output_ = Dense(X_train.shape[1], activation=act_func)(x)\n",
    "\n",
    "model = Model(input_, output_)\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "encoder_model = Model(inputs=model.input, outputs=model.get_layer('feature_vector').output)\n",
    "encoder_model.compile(loss='mse', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "history = model.fit(X_train, X_train, batch_size=batch_size, epochs=epochs, validation_split=0.05, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the Model Training Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history['loss'][15:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('anomaly_detection_full_model.h5')\n",
    "encoder_model.save('anomaly_detection_encoder_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the models**\n",
    "\n",
    "Run this cell if you want to load previously trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('anomaly_detection_full_model.h5')\n",
    "#encoder_model = load_model('anomaly_detection_encoder_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Criteria for Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder network is trained using normal data where it first compresses the input data and then reconstructs the input data. During training the network learns the interactions between various input variables under normal conditions and learns to reconstruct the input variables back to their original values. The reconstruction error is the error is reproducing back the original input values. We will be using `Mean Absolute Error` as our measure for the reconstruction error. The basic idea behind anomaly detection is that the reconstruction error using the trained network for anomalous inputs will be higher than what is typically observed with normal data. \n",
    "\n",
    "Thus, one of the parameters we need to understand is the **threshold for the reconstruction error** that identifies anomalous input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute reconstruction errors for the normal dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will make predictions on the normal dataset, compute the reconstruction error for individual set of inputs, and look that the upper and lower bounds for the reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = clf.transform(normal_df[feature_cols]) # Keep the order, X_train used for training was shuffled\n",
    "X_pred = model.predict(X_train)\n",
    "loss_mae = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "normal_df['loss_mae'] = loss_mae\n",
    "stats = normal_df.loss_mae.describe()\n",
    "whis = 2.0\n",
    "upper_bound = (whis* (stats['75%'] - stats['25%']) + stats['75%'])\n",
    "lower_bound = (-whis* (stats['75%'] - stats['25%']) + stats['25%'])\n",
    "print(('Mean Absolute Error: lower bound: {}, upper bound: {}').format(lower_bound, upper_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the reconstruction errors for the normal dataset**\n",
    "\n",
    "It appears that the threshold value of `0.005` is a reasonable cutoff to identify anomalous input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, sharey=False, sharex=False, figsize=(6, 7))\n",
    "\n",
    "upper_boundary = upper_bound * np.ones(len(loss_mae))\n",
    "\n",
    "ax[0].plot(loss_mae)\n",
    "ax[0].plot(upper_boundary, color='r')\n",
    "sns.boxplot(y=loss_mae, whis=whis, ax = ax[1])\n",
    "\n",
    "ax[0].set_title('Normal Dataset - Line Plot')\n",
    "ax[1].set_title('Normal Dataset - Box Plot')\n",
    "ax[0].set_ylabel('Mean Absolute Error')\n",
    "ax[1].set_ylabel('Mean Absolute Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualize the reconstruction errors for the sudden and gradual datasets**\n",
    "\n",
    "The line plots show the anomalous regions in the two respective datasets.  For the sudden dataset there is an almost instantaneous spike and for the gradual dataset there is a ramp up to the peak error value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sudden = clf.transform(sudden_df[feature_cols])\n",
    "X_sudden_pred = model.predict(X_sudden)\n",
    "loss_mae_sudden = np.mean(np.abs(X_sudden-X_sudden_pred), axis = 1)\n",
    "\n",
    "X_gradual = clf.transform(gradual_df[feature_cols])\n",
    "X_gradual_pred = model.predict(X_gradual)\n",
    "loss_mae_gradual = np.mean(np.abs(X_gradual-X_gradual_pred), axis = 1)\n",
    "\n",
    "sudden_df['loss_mae'] = loss_mae_sudden\n",
    "gradual_df['loss_mae'] = loss_mae_gradual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, sharey=True, sharex=True, figsize=(6, 7))\n",
    "\n",
    "upper_boundary = upper_bound * np.ones(len(loss_mae_sudden))\n",
    "\n",
    "ax[0].plot(loss_mae_sudden)\n",
    "ax[0].plot(upper_boundary, color='r')\n",
    "ax[1].plot(loss_mae_gradual)\n",
    "ax[1].plot(upper_boundary, color='r')\n",
    "\n",
    "ax[0].set_title('Sudden Dataset')\n",
    "ax[1].set_title('Gradual Dataset')\n",
    "ax[0].set_ylabel('Mean Absolute Error')\n",
    "ax[1].set_ylabel('Mean Absolute Error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zoom in to review the trends in the anomalous period**\n",
    "\n",
    "Let’s review the reconstruction errors during the period of May 2019 to August 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudden_test_df = sudden_df.loc[lambda d: (d.date.dt.year == 2019) & \n",
    "                                ((d.date.dt.month == 5) | (d.date.dt.month == 6) | \n",
    "                                 (d.date.dt.month == 7) | (d.date.dt.month == 8)), :]\n",
    "\n",
    "gradual_test_df = gradual_df.loc[lambda d: (d.date.dt.year == 2019) & \n",
    "                                ((d.date.dt.month == 5) | (d.date.dt.month == 6) | \n",
    "                                 (d.date.dt.month == 7) | (d.date.dt.month == 8)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, sharey=True, sharex=True, figsize=(6, 7))\n",
    "\n",
    "upper_boundary = upper_bound * np.ones(len(sudden_test_df))\n",
    "\n",
    "ax[0].plot(sudden_test_df.date, sudden_test_df.loss_mae.values)\n",
    "ax[0].plot(sudden_test_df.date, upper_boundary, color='r')\n",
    "ax[1].plot(sudden_test_df.date, gradual_test_df.loss_mae.values)\n",
    "ax[1].plot(sudden_test_df.date, upper_boundary, color='r')\n",
    "ax[0].set_title('Sudden Dataset')\n",
    "ax[1].set_title('Gradual Dataset')\n",
    "ax[0].set_ylabel('Mean Absolute Error')\n",
    "ax[1].set_ylabel('Mean Absolute Error')\n",
    "plt.xticks(fontsize=8, rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The question is for the gradual case, is there a lower error threshold we can monitor to detect the potential anomaly earlier in the time scale?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consecutive Counts Metric\n",
    "\n",
    "`Consecutive Counts Metric` – is basically computing the number of consecutive errors that are above a given threshold in a real-time feed of time series data. The threshold here will be lower than the threshold monitored for anomalies. The idea is that for normal conditions while you may have occasional points above the lower threshold, that trend may not persist in subsequent readings unless the errors are gradually trending upwards towards anomalous conditions.\n",
    "\n",
    "*Note that this is just one example to predict gradual anomalies earlier, and often you have either change or fine tune your approach to minimize false positives*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consecutive_counts(df, col_name, threshold, start_index = 0):\n",
    "    answer = []\n",
    "    for i in range(start_index, len(df)):\n",
    "        count = 0\n",
    "        current_index = i\n",
    "        current_value = df.iloc[current_index][col_name]\n",
    "        while (current_index >= 0) & (current_value >= threshold):\n",
    "            count = count + 1\n",
    "            current_index = current_index - 1\n",
    "            if (current_index >= 0):\n",
    "                current_value = df.iloc[current_index][col_name]\n",
    "        answer.append(count)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we will use a lower threshold value of **0.003** to compute the consecutive counts metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consecutive_threshold = 0.003\n",
    "normal_df['consecutive_counts'] = consecutive_counts(normal_df, 'loss_mae', consecutive_threshold)\n",
    "sudden_df['consecutive_counts'] = consecutive_counts(sudden_df, 'loss_mae', consecutive_threshold)\n",
    "gradual_df['consecutive_counts'] = consecutive_counts(gradual_df, 'loss_mae', consecutive_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Anomalies\n",
    "\n",
    "With the two established thresholds: **0.005** for point anomalies and **0.003** for gradual (consecutive counts based) anomalies, we will add the two types of predictions to our data sets, standard point anomalies (`anomaly_std`), and anomalies based on consecutive counts metric (`anomaly_cc`). For the case of consecutive counts, we will use **5** consecutive readings above the lower threshold as start of anomalous conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df['anomaly_std'] = normal_df.loss_mae.apply(lambda x: True if x > upper_bound else False)\n",
    "sudden_df['anomaly_std'] = sudden_df.loss_mae.apply(lambda x: True if x > upper_bound else False)\n",
    "gradual_df['anomaly_std'] = gradual_df.loss_mae.apply(lambda x: True if x > upper_bound else False)\n",
    "\n",
    "consecutive_counts_bound = 5\n",
    "normal_df['anomaly_cc'] = normal_df.consecutive_counts.apply(lambda x: True if \n",
    "                                                             x >= consecutive_counts_bound else False)\n",
    "sudden_df['anomaly_cc'] = sudden_df.consecutive_counts.apply(lambda x: True if \n",
    "                                                             x >= consecutive_counts_bound else False)\n",
    "gradual_df['anomaly_cc'] = gradual_df.consecutive_counts.apply(lambda x: True if \n",
    "                                                               x >= consecutive_counts_bound else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review Anomalies in the Normal dataset**\n",
    "\n",
    "The data set shows several point anomalies (`anomaly_std`), and one instance of consecutive counts (`anomaly_cc`) based anomaly around May 16th 2017 that last for two days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe(normal_df[(normal_df.anomaly_std == True) | (normal_df.anomaly_cc == True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review Anomalies in the Sudden dataset**\n",
    "\n",
    "The data set shows a sudden jump in the reconstruction error (`loss_mae`) on May 16 2019 and it persists till end of May. The `anomaly_cc` starts, as expected, on the 4th day after `anomaly_std`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe(sudden_df[(sudden_df.anomaly_std == True) | (sudden_df.anomaly_cc == True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review Anomalies in the Gradual dataset**\n",
    "\n",
    "The `anomaly_cc` starts on June 9th 2019 almost 22 days before the reconstruction error (`loss_mae`) exceeds the normal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe(gradual_df[(gradual_df.anomaly_std == True) | (gradual_df.anomaly_cc == True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Anomalies in the Observed Water Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will visualize the anomalies in the measured water levels during the anomalous period (May-August 2019).\n",
    "\n",
    "As you can observe the **anomaly_std** works best in the case of **sudden anomalies**, whereas, the **anomaly_cc** works best in case of **gradual anomalies**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudden_test_df = sudden_df.loc[lambda d: (d.date.dt.year == 2019) & \n",
    "                                ((d.date.dt.month == 5) | (d.date.dt.month == 6) | \n",
    "                                 (d.date.dt.month == 7) | (d.date.dt.month == 8)), :]\n",
    "\n",
    "gradual_test_df = gradual_df.loc[lambda d: (d.date.dt.year == 2019) & \n",
    "                                ((d.date.dt.month == 5) | (d.date.dt.month == 6) | \n",
    "                                 (d.date.dt.month == 7) | (d.date.dt.month == 8)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 2, sharey=True, sharex=True, figsize=(10, 8))\n",
    "\n",
    "colors_s_1 = ['red' if value == True else 'blue' for value in sudden_test_df.anomaly_std.values]\n",
    "size_s_1 = [10 if value == True else 5 for value in sudden_test_df.anomaly_std.values]\n",
    "colors_s_2 = ['red' if value == True else 'blue' for value in sudden_test_df.anomaly_cc.values]\n",
    "size_s_2 = [10 if value == True else 5 for value in sudden_test_df.anomaly_cc.values]\n",
    "\n",
    "colors_g_1 = ['red' if value == True else 'blue' for value in gradual_test_df.anomaly_std.values]\n",
    "size_g_1 = [10 if value == True else 5 for value in gradual_test_df.anomaly_std.values]\n",
    "colors_g_2 = ['red' if value == True else 'blue' for value in gradual_test_df.anomaly_cc.values]\n",
    "size_g_2 = [10 if value == True else 5 for value in gradual_test_df.anomaly_cc.values]\n",
    "\n",
    "ax[0][0].scatter(sudden_test_df.date, sudden_test_df.water_level, s = size_s_1, c = colors_s_1)\n",
    "ax[0][1].scatter(gradual_test_df.date, gradual_test_df.water_level, s = size_g_1, c = colors_g_1)\n",
    "ax[1][0].scatter(sudden_test_df.date, sudden_test_df.water_level, s = size_s_2, c = colors_s_2)\n",
    "ax[1][1].scatter(gradual_test_df.date, gradual_test_df.water_level, s = size_g_2, c = colors_g_2)\n",
    "\n",
    "ax[0][0].set_title('Sudden Dataset - anomaly_std')\n",
    "ax[1][0].set_title('Sudden Dataset - anomaly_cc')\n",
    "ax[0][1].set_title('Gradual Dataset - anomaly_std')\n",
    "ax[1][1].set_title('Gradual Dataset - anomaly_cc')\n",
    "ax[0][0].set_ylabel('Water Level')\n",
    "ax[1][0].set_ylabel('Water Level')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Normal', markerfacecolor='b', markersize=5), \n",
    "                  Line2D([0], [0], marker='o', color='w', label='Anomaly', markerfacecolor='r', markersize=5)]\n",
    "\n",
    "ax[0][0].legend(handles=legend_elements, frameon=False)\n",
    "#ax[0][1].legend(handles=legend_elements, frameon=False)\n",
    "#ax[1][0].legend(handles=legend_elements, frameon=False)\n",
    "#ax[1][1].legend(handles=legend_elements, frameon=False)\n",
    "\n",
    "f.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Generate the top N principal components of the encoded representation of the input data for both the sudden and gradual datasets during the anomalous periods for each of the respective datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sudden_anomalies = sudden_df.loc[lambda d: (d.date.dt.year == 2019) & (d.date.dt.month == 5), :]\n",
    "\n",
    "gradual_anomalies = gradual_df.loc[lambda d: (d.date.dt.year == 2019) & \n",
    "                                   ((d.date.dt.month == 5) | (d.date.dt.month == 6) | \n",
    "                                    (d.date.dt.month == 7)), :]\n",
    "sudden_anomalies_encoded = encoder_model.predict(clf.transform(sudden_anomalies[feature_cols]))\n",
    "gradual_anomalies_encoded = encoder_model.predict(clf.transform(gradual_anomalies[feature_cols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate principal components for **N = [2, 3, 4, 5]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = [2, 3, 4, 5]\n",
    "sudden_anomalies_pca = []\n",
    "gradual_anomalies_pca = []\n",
    "\n",
    "def pca_analysis(input, results, anomaly_type):\n",
    "    for comp in pca_components: \n",
    "        pca = PCA(n_components = comp)\n",
    "        pca_result = pca.fit_transform(input)\n",
    "        print('{} - Cumulative explained variation for {} principal components: {}'.format(\n",
    "            anomaly_type, comp, np.sum(pca.explained_variance_ratio_)))\n",
    "        results.append(pca_result)\n",
    "\n",
    "pca_analysis(sudden_anomalies_encoded, sudden_anomalies_pca, 'Sudden anomalies')\n",
    "pca_analysis(gradual_anomalies_encoded, gradual_anomalies_pca, 'Gradual anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Principal Components for N = 3\n",
    "\n",
    "Visualize the top 3 principal components of the encoded representation of the input data.\n",
    "\n",
    "The 3-D plots show a clean separation between normal and anomalous points in the sudden dataset, whereas, for the gradual dataset you see a gradual separation at first then followed by a clean separation between the normal and anomalous points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded1 = pd.DataFrame(sudden_anomalies_pca[1], columns=['X','Y', 'Z'])\n",
    "X_embedded1['State'] = np.where(sudden_anomalies.anomaly_std, 'Failure', 'Normal')\n",
    "\n",
    "X_embedded2 = pd.DataFrame(gradual_anomalies_pca[1], columns=['X','Y', 'Z'])\n",
    "X_embedded2['State'] = np.where(gradual_anomalies.anomaly_cc, 'Failure', 'Normal')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax1.set_title('Sudden anomalies readings', y=-0.12)\n",
    "ax2.set_title('Gradual anomalies readings', y=-0.12)\n",
    "\n",
    "colors_1 = ['red' if value == 'Failure' else 'blue' for value in X_embedded1.State.values]\n",
    "ax1.scatter(X_embedded1.X.values, X_embedded1.Y.values, X_embedded1.Z.values, c=colors_1)\n",
    "\n",
    "colors_2 = ['red' if value == 'Failure' else 'blue' for value in X_embedded2.State.values]\n",
    "ax2.scatter(X_embedded2.X.values, X_embedded2.Y.values, X_embedded2.Z.values, c=colors_2)\n",
    "#start, end = ax2.get_xlim()\n",
    "#start, end = ax2.get_ylim()\n",
    "ax2.xaxis.set_ticks(np.arange(-.7, 1.2, 0.4))\n",
    "ax2.yaxis.set_ticks(np.arange(-.8, 0.8, 0.3))\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Normal', markerfacecolor='b', markersize=5), \n",
    "                  Line2D([0], [0], marker='o', color='w', label='Anomaly', markerfacecolor='r', markersize=5)]\n",
    "\n",
    "ax1.legend(handles=legend_elements, loc='upper left', frameon=False)\n",
    "ax2.legend(handles=legend_elements, loc='upper left', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
