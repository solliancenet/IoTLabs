{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection - Polk County, Iowa\n",
    "\n",
    "Anomaly or outlier detection is essentially finding patterns that do not conform to expected behavior. There are several approaches to anomaly detection that are based on either statistical properties, clustering, classification, Principal Component Analysis (PCA), or subsampling. In this notebook we will look at an **autoencoder network** for anomaly or outlier detection. An autoencoder is a neural-net based, unsupervised learning model that is used to learn low-dimensional features that captures some structure underlying the high-dimensional input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Scenario\n",
    "\n",
    "Groundwater level is an important metric, especially for agriculture states such as Iowa. One of the metrics [U.S. Geological Survey (USGS)](https://www.usgs.gov/) monitors is **depth to water level in feet below the land**. In this lab we will use a synthetic dataset that models certain scenarios for Polk County, Iowa. The three key weather-related metrics we will be using are:\n",
    "\n",
    "- water-level (depth to water level in feet below the land)\n",
    "- temperature\n",
    "- humidity\n",
    "\n",
    "The data is generated daily using realistic monthly averages for Polk County, Iowa, for the years 2017 – 2019. The data is generated daily for each of the 92 different sensors/locations within Polk County, Iowa – 3 years x 365 days x 92 sensors = 100,740 total sets of data.\n",
    "\n",
    "We are going to be using 2 copies of the dataset for years 2017 -2019: \n",
    "\n",
    "1. Normal conditions for the county.\n",
    "2. A gradual build up dry conditions in one of the regions in Polk County, Iowa over the months of June and July 2019.\n",
    "\n",
    "The goal of this notebook is to develop an approach to monitor a group of sensors based on their proximity to each other to predict regional anomalies in real-time. We will be grouping the sensors in 6 different location-based clusters as identified by the previous notebook. Thus, in for model training, we will use cluster_id, along with month, temperature, humidity, and water level as our features.\n",
    "\n",
    "To train an autoencoder model that learns the structures in the input data in this more complex scenario will need significant compute resources. In this notebook we will build and train the autoencoder model on local compute and thus at first, we will work with a subset of the data. Once the anomaly detection model and approach are established, in the next notebook we will be leveraging the compute resources provided by Azure Machine Learning service to train the model on the entire dataset. The subset data used in this notebook will range from May 2019 – August 2019 that encompasses the anomalous period of June-July 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. **Setup**: Import required libraries, load the datasets, and create data subsets.\n",
    "\n",
    "2. **Define and Train the Autoencoder Network on local compute**: Use Keras to define and train the autoencoder model.\n",
    "\n",
    "3. **Establish criteria for anomalies**: Define approaches and thresholds for detecting anomalies based on the trained autoencoder model.\n",
    "\n",
    "4. **Predict anomalies**: Used in the trained autoencoder model, make predictions to identify anomalies.\n",
    "\n",
    "5. **Principal Component Analysis**: Apply PCA on the encoded dataset and visualize the data representation at lower dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "import math\n",
    "import timeit\n",
    "from IPython.display import display, HTML, Image, SVG\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "np.random.seed(437)\n",
    "print(\"pandas version: {} numpy version: {}\".format(pd.__version__, np.__version__))\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Model, Sequential, load_model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "print(\"keras version: {} tensorflow version: {} sklearn version: {}\".format(keras.__version__, \n",
    "                                                                        tensorflow.__version__, sklearn.__version__))\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "print('importing libraries done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper method to display a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe(df_in):\n",
    "    s = df_in.style.set_properties(**{'text-align': 'left'})\n",
    "    s.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "    display(HTML(s.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "              'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/normal_multi.xlsx')\n",
    "\n",
    "gradual_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "               'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/gradual_multi.xlsx')\n",
    "\n",
    "normal_df = pd.read_excel(normal_url)\n",
    "gradual_df = pd.read_excel(gradual_url)\n",
    "\n",
    "print('Size of dataset: {} rows'.format(len(normal_df)))\n",
    "print('Done loading datasets!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_df = normal_df.loc[lambda d: (d.date >= '2019-05-01') & (d.date <= '2019-08-31'), :]\n",
    "gradual_df = gradual_df.loc[lambda d: (d.date >= '2019-05-01') & (d.date <= '2019-08-31'), :]\n",
    "print('Size of subset dataset: {} rows'.format(len(normal_df)))\n",
    "print('Done creating subset datasets!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Train the Autoencoder Network on local compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select **cluster_id**, **month**, **temperature**, **humidity**, and **water level** as our features for the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['cluster_id', 'month', 'temperature', 'humidity', 'water_level']\n",
    "categorical = ['cluster_id', 'month']\n",
    "numerical = ['temperature', 'humidity', 'water_level']\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())])) for f in numerical]\n",
    "    \n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = categorical_transformations + numeric_transformations\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations))])\n",
    "\n",
    "X = clf.fit_transform(normal_df[feature_cols])\n",
    "X_train = X\n",
    "np.random.shuffle(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Autoencoder Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(10)\n",
    "set_random_seed(50)\n",
    "act_func = 'elu'\n",
    "\n",
    "input_ = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(100, activation=act_func)(input_)\n",
    "x = Dense(50, activation=act_func)(x)\n",
    "x = Dense(25, activation=act_func)(x)\n",
    "encoder = Dense(12, activation=act_func, name='feature_vector')(x)\n",
    "x = Dense(25, activation=act_func)(encoder)\n",
    "x = Dense(50, activation=act_func)(x)\n",
    "x = Dense(100, activation=act_func)(x)\n",
    "output_ = Dense(X_train.shape[1], activation=act_func)(x)\n",
    "\n",
    "model = Model(input_, output_)\n",
    "lr = 0.001\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "encoder_model = Model(inputs=model.input, outputs=model.get_layer('feature_vector').output)\n",
    "encoder_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "print('')\n",
    "print(model.summary())\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 16\n",
    "\n",
    "print('epochs: ', epochs)\n",
    "print('batch_size: ', batch_size)\n",
    "print('')\n",
    "\n",
    "def schedule(epoch_number, current_lr):\n",
    "    lr = current_lr\n",
    "    if (epoch_number < 25):\n",
    "        lr = 0.001\n",
    "    if (epoch_number >= 25) & (epoch_number < 35):\n",
    "        lr = 0.0005\n",
    "    if (epoch_number >= 35) & (epoch_number < 50):\n",
    "        lr = 0.0003\n",
    "    if (epoch_number >= 50) & (epoch_number < 60):\n",
    "        lr = 0.0001\n",
    "    if (epoch_number >= 60) & (epoch_number < 70):\n",
    "        lr = 0.00008\n",
    "    if (epoch_number >= 70) & (epoch_number < 80):\n",
    "        lr = 0.00006\n",
    "    if (epoch_number >= 80) & (epoch_number < 90):\n",
    "        lr = 0.00004\n",
    "    if (epoch_number >= 90) & (epoch_number < 100):\n",
    "        lr = 0.00002\n",
    "    if (epoch_number >= 100) & (epoch_number < 125):\n",
    "        lr = 0.000009\n",
    "    if (epoch_number >= 125) & (epoch_number < 150):\n",
    "        lr = 0.000007\n",
    "    if (epoch_number >= 150) & (epoch_number < 175):\n",
    "        lr = 0.000005\n",
    "    if (epoch_number >= 175) & (epoch_number < 200):\n",
    "        lr = 0.000001\n",
    "    return lr\n",
    "\n",
    "lr_sch = keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n",
    "\n",
    "print(\"Model training starting...\")\n",
    "start_time = timeit.default_timer()\n",
    "history = model.fit(X_train, X_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[lr_sch], \n",
    "                    verbose=1)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(\"Model training completed.\")\n",
    "print('Elapsed time (min): ', round(elapsed_time/60.0,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save the models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./output/models', exist_ok=True)\n",
    "model.save(os.path.join('./output/models', 'anomaly_detection_multi_full_model.h5'))\n",
    "encoder_model.save(os.path.join('./output/models', 'anomaly_detection_multi_encoder_model.h5'))\n",
    "\n",
    "with open(os.path.join('./output/models', 'history.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "    \n",
    "print(\"Models saved in ./output/models folder\")\n",
    "print(\"Saving model files completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the models**\n",
    "\n",
    "Run this cell if you want to load previously trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model('./output/models/anomaly_detection_multi_full_model.h5')\n",
    "#encoder_model = load_model('./output/models/anomaly_detection_multi_encoder_model.h5')\n",
    "#print('Models loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Criteria for Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder network is trained using normal data where it first compresses the input data and then reconstructs the input data. During training the network learns the interactions between various input variables under normal conditions and learns to reconstruct the input variables back to their original values. The reconstruction error is the error is reproducing back the original input values. We will be using `Mean Absolute Error` as our measure for the reconstruction error. The basic idea behind anomaly detection is that the reconstruction error using the trained network for anomalous inputs will be higher than what is typically observed with normal data. \n",
    "\n",
    "Thus, one of the parameters we need to understand is the **threshold for the reconstruction error** that identifies anomalous input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and compute reconstruction errors for the normal dataset\n",
    "\n",
    "Next, we will make predictions on the normal dataset, compute the reconstruction error for individual set of inputs, and look that the upper and lower bounds for the reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = clf.transform(normal_df[feature_cols]) # Keep the order, X_train used for training was shuffled\n",
    "X_pred = model.predict(X_train)\n",
    "loss_mae = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "normal_df['loss_mae'] = loss_mae\n",
    "stats = normal_df.loss_mae.describe()\n",
    "print(('Max loss mae: {}').format(stats['max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstruction errors for the normal dataset\n",
    "\n",
    "It appears that the threshold value of `0.003` is a reasonable cutoff to identify anomalous input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup upper_bound for anomalous reconstruction error\n",
    "upper_bound = 0.003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "upper_boundary = upper_bound * np.ones(len(normal_df.date.unique()))\n",
    "\n",
    "plt.plot_date(normal_df.date, normal_df.loss_mae, markersize=0.5)\n",
    "plt.plot(normal_df.date.unique(), upper_boundary, color='r')\n",
    "\n",
    "plt.xticks(fontsize=10, rotation=45);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstruction errors for the gradual datasets\n",
    "\n",
    "Make predictions on the gradual dataset, compute the reconstruction error for individual set of inputs. You will see a ramp up in the reconstruction error for some set of data points around June-August 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gradual = clf.transform(gradual_df[feature_cols])\n",
    "X_gradual_pred = model.predict(X_gradual)\n",
    "loss_mae_gradual = np.mean(np.abs(X_gradual-X_gradual_pred), axis = 1)\n",
    "\n",
    "gradual_df['loss_mae'] = loss_mae_gradual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "upper_boundary = upper_bound * np.ones(len(gradual_df.date.unique()))\n",
    "\n",
    "plt.plot_date(gradual_df.date, gradual_df.loss_mae, markersize=0.5)\n",
    "plt.plot(gradual_df.date.unique(), upper_boundary, color='r')\n",
    "\n",
    "plt.xticks(fontsize=10, rotation=45);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstruction errors for the various region clusters in the gradual datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a lower error threshold we can monitor to detect the potential anomaly earlier in the time scale?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = gradual_df.groupby(['date', 'cluster_id'])['water_level', 'loss_mae'].mean()\n",
    "cluster_df.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_upper_bound = 0.0015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "cluster_upper_boundary = cluster_upper_bound * np.ones(len(cluster_df.date.unique()))\n",
    "\n",
    "plt.plot_date(cluster_df.date, cluster_df.loss_mae, markersize=0.5)\n",
    "plt.plot(cluster_df.date.unique(), cluster_upper_boundary, color='r')\n",
    "\n",
    "plt.xticks(fontsize=10, rotation=45);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Anomalies\n",
    "\n",
    "With the two established thresholds: **0.004** for point anomalies and **0.0015** for cluster anomalies, we will add the two types of predictions to our data sets, standard point anomalies (`anomaly_std`), and anomalies based on cluster averages (`anomaly_cluster`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAnomaly(date, cluster_id):\n",
    "    loss = cluster_df.loc[lambda x: (x.date == date) & (x.cluster_id == cluster_id)]['loss_mae'].values[0]\n",
    "    return (True if loss >= cluster_upper_bound else False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This cell will take around 30 seconds to complete**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "gradual_df['anomaly_std'] = gradual_df.loss_mae.apply(lambda x: True if x > upper_bound else False)\n",
    "gradual_df['anomaly_cluster'] = gradual_df.apply(lambda x: isAnomaly(x.date, x.cluster_id), axis = 1)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print('Elapsed time (seconds): ', round(elapsed_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Review Anomalies in the Gradual dataset**\n",
    "\n",
    "The table shows that regional anomalies are predicted for **cluster 1 / North region**. Furthermore, the `anomaly_cluster` starts on June 21st 2019 almost 17 days before the reconstruction error (`loss_mae`) exceeds the normal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataframe(gradual_df[(gradual_df.anomaly_cluster == True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review the reconstruction error for the North region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(8, 5))\n",
    "\n",
    "gradual_test_df = cluster_df.loc[lambda d: (d.date >= '2019-05-01') & (d.date <= '2019-08-31') & \n",
    "                                 (d.cluster_id == 1), :]\n",
    "\n",
    "upper_boundary = cluster_upper_bound * np.ones(len(gradual_test_df))\n",
    "\n",
    "ax.plot(gradual_test_df.loss_mae.values)\n",
    "ax.plot(upper_boundary, color='r')\n",
    "\n",
    "ax.set_title('Gradual Dataset')\n",
    "ax.set_ylabel('Mean Absolute Error');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Anomalies in the Observed Water Levels in the North region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will visualize the anomalies in the measured water levels during the anomalous period (June-August 2019). As you can see the cluster anomalies are established well before the point anomalies show persistance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradual_test_df = gradual_df.loc[lambda d: (d.date.dt.year == 2019) & (d.cluster_id == 1) & \n",
    "                                ((d.date.dt.month == 6) | (d.date.dt.month == 7) | (d.date.dt.month == 8)), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(2, 1, sharey=True, sharex=True, figsize=(7, 5))\n",
    "\n",
    "colors_g_1 = ['red' if value == True else 'blue' for value in gradual_test_df.anomaly_std.values]\n",
    "size_g_1 = [10 if value == True else 5 for value in gradual_test_df.anomaly_std.values]\n",
    "colors_g_2 = ['red' if value == True else 'blue' for value in gradual_test_df.anomaly_cluster.values]\n",
    "size_g_2 = [10 if value == True else 5 for value in gradual_test_df.anomaly_cluster.values]\n",
    "\n",
    "ax[0].scatter(gradual_test_df.date, gradual_test_df.water_level, s = size_g_1, c = colors_g_1)\n",
    "ax[1].scatter(gradual_test_df.date, gradual_test_df.water_level, s = size_g_2, c = colors_g_2)\n",
    "\n",
    "ax[0].set_title('Gradual Dataset - anomaly_std')\n",
    "ax[1].set_title('Gradual Dataset - anomaly_cluster')\n",
    "\n",
    "ax[0].set_ylabel('Water Level')\n",
    "ax[1].set_ylabel('Water Level')\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Normal', markerfacecolor='b', markersize=5), \n",
    "                  Line2D([0], [0], marker='o', color='w', label='Anomaly', markerfacecolor='r', markersize=5)]\n",
    "\n",
    "ax[0].legend(handles=legend_elements, frameon=False)\n",
    "#ax[1].legend(handles=legend_elements, frameon=False)\n",
    "\n",
    "ax[1].xaxis.set_ticks(['2019-06-01', '2019-06-15', '2019-07-01', '2019-07-15', \n",
    "                       '2019-08-01', '2019-08-15', '2019-08-31'])\n",
    "\n",
    "plt.xticks(fontsize=10, rotation=25);\n",
    "\n",
    "f.tight_layout(rect=[0, 0.03, 1, 0.95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "Generate the top N principal components of the encoded representation of the input data for the gradual dataset during the anomalous period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradual_anomalies = gradual_df.loc[lambda d: (d.date.dt.year == 2019) & (d.cluster_id == 1) & \n",
    "                                   ((d.date.dt.month == 5) | (d.date.dt.month == 6) | \n",
    "                                    (d.date.dt.month == 7) | (d.date.dt.month == 8)), :]\n",
    "gradual_anomalies_encoded = encoder_model.predict(clf.transform(gradual_anomalies[feature_cols]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate principal components for **N = [2, 3, 4, 5]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_components = [2, 3, 4, 5]\n",
    "gradual_anomalies_pca = []\n",
    "\n",
    "def pca_analysis(input, results, anomaly_type):\n",
    "    for comp in pca_components: \n",
    "        pca = PCA(n_components = comp)\n",
    "        pca_result = pca.fit_transform(input)\n",
    "        print('{} - Cumulative explained variation for {} principal components: {}'.format(\n",
    "            anomaly_type, comp, np.sum(pca.explained_variance_ratio_)))\n",
    "        results.append(pca_result)\n",
    "\n",
    "pca_analysis(gradual_anomalies_encoded, gradual_anomalies_pca, 'Gradual anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Principal Components for N = 3\n",
    "\n",
    "Visualize the top 3 principal components of the encoded representation of the input data.\n",
    "\n",
    "Looking at the 3-D plots, it appears that the second principal component is largely sufficient to predict anomalies. The anomalous or near-anomalous data lies in the region **x_2 > 1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded1 = pd.DataFrame(gradual_anomalies_pca[1], columns=['X','Y', 'Z'])\n",
    "X_embedded1['State'] = np.where(gradual_anomalies.anomaly_std, 'Failure', 'Normal')\n",
    "\n",
    "X_embedded2 = pd.DataFrame(gradual_anomalies_pca[1], columns=['X','Y', 'Z'])\n",
    "X_embedded2['State'] = np.where(gradual_anomalies.anomaly_cluster, 'Failure', 'Normal')\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax1 = fig.add_subplot(121, projection='3d')\n",
    "ax2 = fig.add_subplot(122, projection='3d')\n",
    "\n",
    "ax1.set_title('Anomoly Std', y=-0.12)\n",
    "ax2.set_title('Anomoly Cluster', y=-0.12)\n",
    "\n",
    "colors_1 = ['red' if value == 'Failure' else 'blue' for value in X_embedded1.State.values]\n",
    "ax1.scatter(X_embedded1.X.values, X_embedded1.Y.values, X_embedded1.Z.values, c=colors_1)\n",
    "\n",
    "colors_2 = ['red' if value == 'Failure' else 'blue' for value in X_embedded2.State.values]\n",
    "ax2.scatter(X_embedded2.X.values, X_embedded2.Y.values, X_embedded2.Z.values, c=colors_2)\n",
    "\n",
    "#start, end = ax2.get_xlim()\n",
    "#print(start, end)\n",
    "\n",
    "ax1.xaxis.set_ticks([-1, 0, 1, 2])\n",
    "ax1.yaxis.set_ticks([-1, 0, 1, 2])\n",
    "ax1.zaxis.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "ax2.xaxis.set_ticks([-1, 0, 1, 2])\n",
    "ax2.yaxis.set_ticks([-1, 0, 1, 2])\n",
    "ax2.zaxis.set_ticks([-1, -0.5, 0, 0.5, 1])\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "legend_elements = [Line2D([0], [0], marker='o', color='w', label='Normal', markerfacecolor='b', markersize=5), \n",
    "                  Line2D([0], [0], marker='o', color='w', label='Anomaly', markerfacecolor='r', markersize=5)]\n",
    "\n",
    "ax1.legend(handles=legend_elements, loc='upper left', frameon=False)\n",
    "ax2.legend(handles=legend_elements, loc='upper left', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
