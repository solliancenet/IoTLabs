{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection - Azure Machine Learning service\n",
    "\n",
    "Anomaly or outlier detection is essentially finding patterns that do not conform to expected behavior. There are several approaches to anomaly detection that are based on either statistical properties, clustering, classification, Principal Component Analysis (PCA), or subsampling. In this notebook we will look at an **autoencoder network** for anomaly or outlier detection. An autoencoder is a neural-net based, unsupervised learning model that is used to learn low-dimensional features that captures some structure underlying the high-dimensional input data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Scenario\n",
    "\n",
    "Groundwater level is an important metric, especially for agriculture states such as Iowa. One of the metrics [U.S. Geological Survey (USGS)](https://www.usgs.gov/) monitors is **depth to water level in feet below the land**. In this lab we will use a synthetic dataset that models certain scenarios for Polk County, Iowa. The three key weather-related metrics we will be using are:\n",
    "\n",
    "- water-level (depth to water level in feet below the land)\n",
    "- temperature\n",
    "- humidity\n",
    "\n",
    "The data is generated daily using realistic monthly averages for Polk County, Iowa, for the years 2017 – 2019. The data is generated daily for each of the 92 different sensors/locations within Polk County, Iowa – 3 years x 365 days x 92 sensors = 100,740 total sets of data.\n",
    "\n",
    "We are going to be using 2 copies of the dataset for years 2017 -2019: \n",
    "\n",
    "1. Normal conditions for the county.\n",
    "2. A gradual build up dry conditions in one of the regions in Polk County, Iowa over the months of June and July 2019.\n",
    "\n",
    "The goal of this notebook is to develop an approach to monitor a group of sensors based on their proximity to each other to predict regional anomalies in real-time. We will be grouping the sensors in 6 different location-based clusters as identified by the previous notebook. Thus, in for model training, we will use cluster_id, along with month, temperature, humidity, and water level as our features.\n",
    "\n",
    "To train an autoencoder model that learns the structures in the input data in this more complex scenario will need significant compute resources. Thus, in the notebook we will be leveraging the compute resources provided by **Azure Machine Learning service** for our model training.\n",
    "\n",
    "After training the model, we will register the model with Azure Machine Learning Workspace, then package and deploy the model as a webservice to make predictions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. **Setup**: Import required libraries, load the datasets, and create the Azure Machine Learning Workspace.\n",
    "\n",
    "2. **Remotely Train the Autoencoder Network using the Azure ML Compute**: Create the Azure Machine Learning compute, and submit the training job.\n",
    "\n",
    "3. **Establish criteria for anomalies**: Define approaches and thresholds for detecting anomalies based on the trained autoencoder model.\n",
    "\n",
    "4. **Deploy Model to Azure Container Instance (ACI) as a Web Service**: Get the registered models from the Azure Machine Learning Workspace, create the scoring script, package and deploy the model for making predictions.\n",
    "\n",
    "5. **Test Deployment**: Test the model deployment by making calls directly on the service object and also by calling the deployed webservice over HTTP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Experiment, Workspace, Run, Datastore\n",
    "from azureml.core.compute import ComputeTarget\n",
    "from azureml.core.model import Model\n",
    "from azureml.train.dnn import TensorFlow\n",
    "from azureml.train.estimator import Estimator\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "print(\"Azure ML SDK version:\", azureml.core.VERSION)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import os\n",
    "import math\n",
    "import timeit\n",
    "from IPython.display import display, HTML, Image, SVG\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "print(\"pandas version: {} numpy version: {}\".format(pd.__version__, np.__version__))\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.cluster import KMeans\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.models import Model as KModel # resolve name conflict with Azure Model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "print(\"keras version: {} tensorflow version: {} sklearn version: {}\".format(keras.__version__, \n",
    "                                                                        tensorflow.__version__, sklearn.__version__))\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd\n",
    "from geopandas import GeoDataFrame\n",
    "\n",
    "print('importing libraries done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper method to display a pandas dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_dataframe(df_in):\n",
    "    s = df_in.style.set_properties(**{'text-align': 'left'})\n",
    "    s.set_table_styles([dict(selector='th', props=[('text-align', 'left')])])\n",
    "    display(HTML(s.render()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "              'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/normal_multi.xlsx')\n",
    "\n",
    "gradual_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "               'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/gradual_multi.xlsx')\n",
    "\n",
    "normal_df = pd.read_excel(normal_url)\n",
    "gradual_df = pd.read_excel(gradual_url)\n",
    "\n",
    "print('Size of dataset: {} rows'.format(len(normal_df)))\n",
    "print('Done loading datasets!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Machine Learning service setup\n",
    "To begin, you will need to provide the following information about your Azure Subscription.\n",
    "\n",
    "**If you are using your own Azure subscription, please provide names for subscription_id, resource_group, workspace_name and workspace_region to use.** Note that the workspace needs to be of type [Machine Learning Workspace](https://docs.microsoft.com/en-us/azure/machine-learning/service/setup-create-workspace).\n",
    "\n",
    "**If an environment is provided to you be sure to replace XXXXX in the values below with your unique identifier.**\n",
    "\n",
    "In the following cell, be sure to set the values for `subscription_id`, `resource_group`, `workspace_name` and `workspace_region` as directed by the comments (*these values can be acquired from the Azure Portal*).\n",
    "\n",
    "To get these values, do the following:\n",
    "1. Navigate to the Azure Portal and login with the credentials provided.\n",
    "2. From the left hand menu, under Favorites, select `Resource Groups`.\n",
    "3. In the list, select the resource group with the name similar to `XXXXX`.\n",
    "4. From the Overview tab, capture the desired values.\n",
    "\n",
    "Execute the following cell by selecting the `>|Run` button in the command bar above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provide the Subscription ID of your existing Azure subscription\n",
    "subscription_id = \"\" # <- needs to be the subscription with the Quick-Starts resource group\n",
    "\n",
    "#Provide values for the existing Resource Group \n",
    "resource_group = \"Quick-Starts-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "\n",
    "#Provide the Workspace Name and Azure Region of the Azure Machine Learning Workspace\n",
    "workspace_name = \"quick-starts-ws-XXXXX\" # <- replace XXXXX with your unique identifier\n",
    "workspace_region = \"eastus\" # <- region of your Quick-Starts resource group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and connect to an Azure Machine Learning Workspace\n",
    "\n",
    "Run the following cell to create a new Azure Machine Learning **Workspace** and save the configuration to disk (next to the Jupyter notebook). \n",
    "\n",
    "**Important Note**: You will be prompted to login in the text that is output below the cell. Be sure to navigate to the URL displayed and enter the code that is provided. Once you have entered the code, return to this notebook and wait for the output to read `Workspace configuration succeeded`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.create(\n",
    "    name = workspace_name,\n",
    "    subscription_id = subscription_id,\n",
    "    resource_group = resource_group, \n",
    "    location = workspace_region,\n",
    "    exist_ok = True)\n",
    "\n",
    "ws.write_config()\n",
    "print('Workspace configuration succeeded')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remotely Train the Autoencoder Network using the Azure ML Compute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cells, you will *not* train the model against the data you just downloaded using the resources provided by Azure Notebooks. Instead, you will deploy an Azure ML Compute cluster that will download the data and use a trainings script to train the model. All of the training will be performed remotely with respect to this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create AML Compute Cluster\n",
    "\n",
    "Azure Machine Learning Compute is a service for provisioning and managing clusters of Azure virtual machines for running machine learning workloads. Let's create a new Aml Compute in the current workspace, if it doesn't already exist. We will run the model training jobs on this compute target. This will take couple of minutes to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create AML CPU based Compute Cluster\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "cluster_name = \"amlcompute-ad\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='Standard_NC6',\n",
    "                                                           min_nodes=1, max_nodes=1)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)\n",
    "\n",
    "# Use the 'status' property to get a detailed status for the current AmlCompute. \n",
    "print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training script"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training script downloads the data, defines and trains the autoencoder model. It saves the model files and also **registers** the trained models with Azure Machine Learning service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_file_folder = './scripts'\n",
    "script_file_name = 'train.py'\n",
    "script_file_full_path = os.path.join(script_file_folder, script_file_name)\n",
    "os.makedirs(script_file_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_file_full_path\n",
    "import os\n",
    "import math\n",
    "import timeit\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(437)\n",
    "\n",
    "print(\"pandas version: {} numpy version: {}\".format(pd.__version__, np.__version__))\n",
    "\n",
    "import sklearn\n",
    "from sklearn import preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.layers.core import Dense \n",
    "from keras.models import Sequential, load_model\n",
    "from keras.models import Model as KModel # resolve name conflict with Azure Model\n",
    "from keras import regularizers\n",
    "from keras.models import model_from_json\n",
    "from numpy.random import seed\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "from azureml.core import Run\n",
    "from azureml.core.model import Model\n",
    "\n",
    "print(\"keras version: {} tensorflow version: {} sklearn version: {}\".format(keras.__version__, \n",
    "                                                                            tensorflow.__version__, \n",
    "                                                                            sklearn.__version__))\n",
    "\n",
    "print(\"Loading data file.\")\n",
    "normal_url = ('https://quickstartsws9073123377.blob.core.windows.net/'\n",
    "              'azureml-blobstore-0d1c4218-a5f9-418b-bf55-902b65277b85/anomaly_detection/normal_multi.xlsx')\n",
    "\n",
    "normal_df = pd.read_excel(normal_url)\n",
    "print(\"Loading data file completed.\")\n",
    "\n",
    "feature_cols = ['cluster_id', 'month', 'temperature', 'humidity', 'water_level']\n",
    "categorical = ['cluster_id', 'month']\n",
    "numerical = ['temperature', 'humidity', 'water_level']\n",
    "\n",
    "numeric_transformations = [([f], Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', MinMaxScaler())])) for f in numerical]\n",
    "    \n",
    "categorical_transformations = [([f], OneHotEncoder(handle_unknown='ignore', sparse=False)) for f in categorical]\n",
    "\n",
    "transformations = categorical_transformations + numeric_transformations\n",
    "\n",
    "clf = Pipeline(steps=[('preprocessor', DataFrameMapper(transformations))])\n",
    "\n",
    "X = clf.fit_transform(normal_df[feature_cols])\n",
    "X_train = X\n",
    "np.random.shuffle(X_train)\n",
    "\n",
    "seed(10)\n",
    "set_random_seed(50)\n",
    "act_func = 'elu'\n",
    "\n",
    "input_ = Input(shape=(X_train.shape[1],))\n",
    "x = Dense(100, activation=act_func)(input_)\n",
    "x = Dense(50, activation=act_func)(x)\n",
    "x = Dense(25, activation=act_func)(x)\n",
    "encoder = Dense(12, activation=act_func, name='feature_vector')(x)\n",
    "x = Dense(25, activation=act_func)(encoder)\n",
    "x = Dense(50, activation=act_func)(x)\n",
    "x = Dense(100, activation=act_func)(x)\n",
    "output_ = Dense(X_train.shape[1], activation=act_func)(x)\n",
    "\n",
    "model = KModel(input_, output_)\n",
    "lr = 0.001\n",
    "opt = keras.optimizers.Adam(lr=lr)\n",
    "model.compile(loss='mse', optimizer=opt)\n",
    "\n",
    "encoder_model = KModel(inputs=model.input, outputs=model.get_layer('feature_vector').output)\n",
    "encoder_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "epochs = 200\n",
    "batch_size = 16\n",
    "\n",
    "print('')\n",
    "print(model.summary())\n",
    "print('')\n",
    "print('lr: ', lr)\n",
    "print('epochs: ', epochs)\n",
    "print('batch_size: ', batch_size)\n",
    "print('')\n",
    "\n",
    "def schedule(epoch_number, current_lr):\n",
    "    lr = current_lr\n",
    "    if (epoch_number < 25):\n",
    "        lr = 0.001\n",
    "    if (epoch_number >= 25) & (epoch_number < 35):\n",
    "        lr = 0.0005\n",
    "    if (epoch_number >= 35) & (epoch_number < 50):\n",
    "        lr = 0.0003\n",
    "    if (epoch_number >= 50) & (epoch_number < 60):\n",
    "        lr = 0.0001\n",
    "    if (epoch_number >= 60) & (epoch_number < 70):\n",
    "        lr = 0.00008\n",
    "    if (epoch_number >= 70) & (epoch_number < 80):\n",
    "        lr = 0.00006\n",
    "    if (epoch_number >= 80) & (epoch_number < 90):\n",
    "        lr = 0.00004\n",
    "    if (epoch_number >= 90) & (epoch_number < 100):\n",
    "        lr = 0.00002\n",
    "    if (epoch_number >= 100) & (epoch_number < 125):\n",
    "        lr = 0.000009\n",
    "    if (epoch_number >= 125) & (epoch_number < 150):\n",
    "        lr = 0.000007\n",
    "    if (epoch_number >= 150) & (epoch_number < 175):\n",
    "        lr = 0.000005\n",
    "    if (epoch_number >= 175) & (epoch_number < 200):\n",
    "        lr = 0.000001\n",
    "    return lr\n",
    "\n",
    "lr_sch = keras.callbacks.LearningRateScheduler(schedule, verbose=1)\n",
    "\n",
    "print(\"Model training starting...\")\n",
    "start_time = timeit.default_timer()\n",
    "history = model.fit(X_train, X_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=epochs, \n",
    "                    validation_split=0.2, \n",
    "                    callbacks=[lr_sch], \n",
    "                    verbose=2)\n",
    "elapsed_time = timeit.default_timer() - start_time\n",
    "print(\"Model training completed.\")\n",
    "print('Elapsed time (min): ', round(elapsed_time/60.0,0))\n",
    "\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "# save the models\n",
    "model.save(os.path.join('./outputs', 'anomaly_detection_multi_full_model.h5'))\n",
    "encoder_model.save(os.path.join('./outputs', 'anomaly_detection_multi_encoder_model.h5'))\n",
    "\n",
    "# save the feature preprocessor pipeline\n",
    "pickle.dump(clf, open(os.path.join('./outputs', 'preprocessor.pkl'), 'wb'))\n",
    "\n",
    "# save training history\n",
    "with open(os.path.join('./outputs', 'history.txt'), 'w') as f:\n",
    "    f.write(str(history.history))\n",
    "\n",
    "print(\"Models saved in ./outputs folder\")\n",
    "print(\"Saving model files completed.\")\n",
    "\n",
    "\n",
    "# Register the Models\n",
    "run = Run.get_context()\n",
    "\n",
    "os.chdir(\"./outputs\")\n",
    "\n",
    "model_path = 'anomaly_detection_multi_full_model.h5'\n",
    "model_name = 'anomaly_detector'\n",
    "model_description = 'Autoencoder network for anomaly detection.'\n",
    "model = Model.register(\n",
    "    model_path=model_path,  # this points to a local file\n",
    "    model_name=model_name,  # this is the name the model is registered as\n",
    "    tags={\"type\": \"autoencoder\", \"run_id\": run.id},\n",
    "    description=model_description,\n",
    "    workspace=run.experiment.workspace\n",
    ")\n",
    "\n",
    "preprocessor_path = 'preprocessor.pkl'\n",
    "preprocessor_name = 'preprocessor'\n",
    "preprocessor_description = 'Feature preprocessor pipeline for anomaly detection.'\n",
    "preprocessor = Model.register(\n",
    "    model_path=preprocessor_path,  # this points to a local file\n",
    "    model_name=preprocessor_name,  # this is the name the model is registered as\n",
    "    tags={\"type\": \"preprocessor\", \"run_id\": run.id},\n",
    "    description=preprocessor_description,\n",
    "    workspace=run.experiment.workspace\n",
    ")\n",
    "\n",
    "os.chdir(\"..\")\n",
    "\n",
    "print(\"Model registered: {} \\nModel Description: {} \\nModel Version: {}\".format(model.name, \n",
    "                                                                                model.description, \n",
    "                                                                                model.version))\n",
    "\n",
    "print(\"Feature preprocessor pipeline registered: {} \\nDescription: {} \\nVersion: {}\".format(\n",
    "    preprocessor.name, \n",
    "    preprocessor.description, \n",
    "    preprocessor.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Keras estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_est = TensorFlow(source_directory=script_file_folder,\n",
    "                       compute_target=compute_target,\n",
    "                       entry_script=script_file_name, \n",
    "                       conda_packages=['numpy==1.16.4', 'xlrd==1.2.0', 'pandas==0.25.1', 'scikit-learn==0.21.3'], \n",
    "                       pip_packages=['sklearn-pandas==1.8.0', 'keras==2.2.5'], \n",
    "                       framework_version='1.13')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit the training run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code pattern to submit a training run to Azure Machine Learning compute targets is always:\n",
    "\n",
    "- Create an experiment to run.\n",
    "- Submit the experiment.\n",
    "- Wait for the run to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'anomaly-detection-lab3'\n",
    "experiment = Experiment(ws, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submit the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that experiment run will perform the following:\n",
    "\n",
    "- Build and deploy the container to Azure Machine Learning compute (~8 minutes)\n",
    "- Execute the training script (~22 minutes)\n",
    "\n",
    "If you change only the training script and re-submit, it will run faster the second time because the necessary container is already prepared so the time requried is just that for executing the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(keras_est, tags = {\"type\": \"anomaly-detection\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor the Run\n",
    "\n",
    "Using the azureml Jupyter widget, you can monitor the training run. This will approximately take around 30 minutes to complete. Once the training is completed you can then download the trained models locally by running the **Download the trained models** cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an output folder in the current directory\n",
    "os.makedirs('./outputs', exist_ok=True)\n",
    "\n",
    "for f in run.get_file_names():\n",
    "    if f.startswith('outputs'):\n",
    "        output_file_path = os.path.join('./outputs', f.split('/')[-1])\n",
    "        print('Downloading from {} to {} ...'.format(f, output_file_path))\n",
    "        run.download_file(name=f, output_file_path=output_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained models\n",
    "model = load_model(os.path.join('./outputs', 'anomaly_detection_multi_full_model.h5'))\n",
    "encoder_model = load_model(os.path.join('./outputs', 'anomaly_detection_multi_encoder_model.h5'))\n",
    "clf = pickle.load(open(os.path.join('./outputs', 'preprocessor.pkl'), 'rb'))\n",
    "print('Models loaded!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Establish Criteria for Anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoder network is trained using normal data where it first compresses the input data and then reconstructs the input data. During training the network learns the interactions between various input variables under normal conditions and learns to reconstruct the input variables back to their original values. The reconstruction error is the error is reproducing back the original input values. We will be using `Mean Absolute Error` as our measure for the reconstruction error. The basic idea behind anomaly detection is that the reconstruction error using the trained network for anomalous inputs will be higher than what is typically observed with normal data. \n",
    "\n",
    "Thus, one of the parameters we need to understand is the **threshold for the reconstruction error** that identifies anomalous input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and compute reconstruction errors for the normal dataset\n",
    "\n",
    "Next, we will make predictions on the normal dataset, compute the reconstruction error for individual set of inputs, and look that the upper and lower bounds for the reconstruction errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['cluster_id', 'month', 'temperature', 'humidity', 'water_level']\n",
    "X_train = clf.transform(normal_df[feature_cols]) # Keep the order\n",
    "X_pred = model.predict(X_train)\n",
    "loss_mae = np.mean(np.abs(X_pred-X_train), axis = 1)\n",
    "normal_df['loss_mae'] = loss_mae\n",
    "stats = normal_df.loss_mae.describe()\n",
    "print(('Max loss mae: {}').format(stats['max']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstruction errors for the normal dataset\n",
    "\n",
    "It appears that the threshold value of `0.001` is a reasonable cutoff to identify anomalous input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup upper_bound for anomalous reconstruction error\n",
    "upper_bound = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "upper_boundary = upper_bound * np.ones(len(normal_df.date.unique()))\n",
    "\n",
    "plt.plot_date(normal_df.date, normal_df.loss_mae, markersize=0.5)\n",
    "plt.plot(normal_df.date.unique(), upper_boundary, color='r')\n",
    "\n",
    "plt.xticks(fontsize=10, rotation=45);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstruction errors for the gradual datasets\n",
    "\n",
    "Make predictions on the gradual dataset, compute the reconstruction error for individual set of inputs. You will see a ramp up in the reconstruction error for some set of data points around June-August 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_gradual = clf.transform(gradual_df[feature_cols])\n",
    "X_gradual_pred = model.predict(X_gradual)\n",
    "loss_mae_gradual = np.mean(np.abs(X_gradual-X_gradual_pred), axis = 1)\n",
    "\n",
    "gradual_df['loss_mae'] = loss_mae_gradual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "upper_boundary = upper_bound * np.ones(len(gradual_df.date.unique()))\n",
    "\n",
    "plt.plot_date(gradual_df.date, gradual_df.loss_mae, markersize=0.5)\n",
    "plt.plot(gradual_df.date.unique(), upper_boundary, color='r')\n",
    "\n",
    "plt.xticks(fontsize=10, rotation=45);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the reconstruction errors for the various region clusters in the gradual datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is there a lower error threshold we can monitor to detect the potential anomaly earlier in the time scale?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df = gradual_df.groupby(['date', 'cluster_id'])['water_level', 'loss_mae'].mean()\n",
    "cluster_df.reset_index(drop=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_upper_bound = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "\n",
    "cluster_upper_boundary = cluster_upper_bound * np.ones(len(cluster_df.date.unique()))\n",
    "\n",
    "plt.plot_date(cluster_df.date, cluster_df.loss_mae, markersize=0.5)\n",
    "plt.plot(cluster_df.date.unique(), cluster_upper_boundary, color='r')\n",
    "\n",
    "plt.xticks(fontsize=10, rotation=45);\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy Anomaly Detector Model as Webservice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Registered Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'anomaly_detector'\n",
    "preprocessor_name = 'preprocessor'\n",
    "\n",
    "registered_model = Model(ws, name = model_name)\n",
    "registered_preprocessor = Model(ws, name = preprocessor_name)\n",
    "\n",
    "print(registered_model)\n",
    "print()\n",
    "print(registered_preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Scoring Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "from azureml.core.model import Model\n",
    "\n",
    "columns = ['cluster_id', 'month', 'temperature', 'humidity', 'water_level']\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    global clf\n",
    "    \n",
    "    print('Get model paths...')\n",
    "    model_path = Model.get_model_path('anomaly_detector')\n",
    "    preprocessor_path = Model.get_model_path('preprocessor')\n",
    "    print(model_path)\n",
    "    print(preprocessor_path)\n",
    "\n",
    "    print('Load models...')\n",
    "    model = load_model(model_path)\n",
    "    clf = pickle.load(open(preprocessor_path, 'rb'))\n",
    "    print('Done loading models!')\n",
    "    \n",
    "def run(input_json):\n",
    "    # Get predictions for each data point\n",
    "    print('Create dataframe')\n",
    "    inputs = json.loads(input_json)\n",
    "    data_df = pd.DataFrame(np.array(inputs).reshape(-1, len(columns)), columns = columns)\n",
    "    \n",
    "    print('Make predictions')\n",
    "    X = clf.transform(data_df)\n",
    "    X_pred = model.predict(X)\n",
    "    \n",
    "    print('Calcuate loss_mae')\n",
    "    loss_mae = np.mean(np.abs(X_pred-X), axis = 1)\n",
    "    data_df['loss_mae'] = np.round(loss_mae, 5)\n",
    "    \n",
    "    print('Calcuate anomaly_std')\n",
    "    upper_bound = 0.001    \n",
    "    data_df['anomaly_std'] = data_df.loss_mae.apply(lambda x: True if x > upper_bound else False)\n",
    "    \n",
    "    # You can return any data type as long as it is JSON-serializable\n",
    "    print('Return results')\n",
    "    return {'loss_mae': data_df.loss_mae.values.tolist(), 'anomaly_std': data_df.anomaly_std.values.tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Package Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Conda dependencies environment file\n",
    "print(\"Creating conda dependencies file locally...\")\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "conda_packages = ['numpy==1.16.4', 'xlrd==1.2.0', 'pandas==0.25.1', 'scikit-learn==0.21.3']\n",
    "pip_packages = ['azureml-sdk', 'sklearn-pandas==1.8.0', 'tensorflow==1.13.1', 'keras==2.2.5']\n",
    "mycondaenv = CondaDependencies.create(conda_packages=conda_packages, pip_packages=pip_packages)\n",
    "\n",
    "conda_file = 'dependencies.yml'\n",
    "with open(conda_file, 'w') as f:\n",
    "    f.write(mycondaenv.serialize_to_string())\n",
    "\n",
    "runtime = 'python'\n",
    "\n",
    "# create container image configuration\n",
    "print(\"Creating container image configuration...\")\n",
    "from azureml.core.image import ContainerImage\n",
    "image_config = ContainerImage.image_configuration(execution_script = 'score.py', \n",
    "                                                  runtime = runtime, \n",
    "                                                  conda_file = conda_file)\n",
    "\n",
    "# create the image\n",
    "image_name = 'anomaly-detector-image'\n",
    "\n",
    "from azureml.core import Image\n",
    "image = Image.create(name=image_name, \n",
    "                     models=[registered_model, registered_preprocessor], \n",
    "                     image_config=image_config, \n",
    "                     workspace=ws)\n",
    "\n",
    "# wait for image creation to finish\n",
    "image.wait_for_creation(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy Model to Azure Container Instance (ACI) as a Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice, Webservice\n",
    "\n",
    "aci_name = 'anomaly-detector01'\n",
    "\n",
    "aci_config = AciWebservice.deploy_configuration(\n",
    "    cpu_cores = 1, \n",
    "    memory_gb = 1, \n",
    "    tags = {'name': aci_name}, \n",
    "    description = 'Autoencoder network for anomaly detection.')\n",
    "\n",
    "service_name = 'anomaly-detector-service'\n",
    "\n",
    "aci_service = Webservice.deploy_from_image(deployment_config=aci_config, \n",
    "                                           image=image, \n",
    "                                           name=service_name, \n",
    "                                           workspace=ws)\n",
    "\n",
    "aci_service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make direct calls on the service object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# ['cluster_id', 'month', 'temperature', 'humidity', 'water_level']\n",
    "data = [[1.0, 7.0, 79.6, 68.2, 15.7], [1.0, 7.0, 72.6, 68.2, 18.8]]\n",
    "\n",
    "result = aci_service.run(json.dumps(data))\n",
    "print('Predictions')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make HTTP calls to test the deployed Web Service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = aci_service.scoring_uri\n",
    "print('ACI Service: {} scoring URI is: {}'.format(aci_name, url))\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "response = requests.post(url, json.dumps(data), headers=headers)\n",
    "print('Predictions')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
